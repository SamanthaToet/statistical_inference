---
title: "Statistical Inference"
author: "Samantha Toet"
date: "11/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intro

Generating conclusions about a population from a noisy sample.

A statistic (singular) is a number computed from a sample of data, and a random variable is an outcome from an experiment.

Deterministic processes, such as computing means or variances, applied to random variables, produce additional random variables which have their own distributions.

Two broad flavors of inference:

1. Frequency - long run proportion of times an event occurs in independent, identically distributed repetitions.
2. Bayesian - the probability estimate for a hypothesis is updated as additional evidence is acquired.

## Probability 

Probability is a function that takes a possible outcome from an expiriment and assigns it a number between 0 and 1. 

The probability that something occurs is 1 (the die must be rolled)

The probability of the union of any two sets of outcomes that have nothing in common (mutually exclusive) is the sum of their respectie probabilities. 

For example: 

* If A means you roll a 1 and a 2 and B means you roll a 4 and a 4, A and B cannot both occur. 
* P(A $\cup$ B) = P(A) + P(B)

Rules that probability must follow:

* The probability that nothing occurs is 0
* The probability that something occurs is 1
* The probability of something is 1 minus the probability that the opposite occurs 
* The probability of at least 1 of 2 (or more) things that can not simultaneously occur (mututally exclusive) is the sum of their respective probabilities. 
* If an event A implies the occurence of event B, then the probability of A occuring is less than the probability that B occurs (think of A as being inside B)
* For any 2 events the probability that at least one occurs is the sum of their probabilities minus their intersection 

### Swirl examples 
The probability of rolling a fair dice twice and getting a 4 both times is 1/6 * 1/6 = 1/36

The probability of rolling a fair dice twice and getting the same number each time is 1/6 (since we don't care about the outcome of the first roll the probability is 1, the second roll has to match the first so it's probability is 1/6 so the probability of rolling the same number twice is 1 * 1/6 = 1/6)

If you roll 2 dice (1 red 1 green), there are 36 different outcomes. The probability of rolling a 10 is 3/36. 

The probability of at least one of two events, A and B, occurring is the sum of their individual probabilities minus the probability of their intersection. P(A U B) = P(A) + P(B) - P(A&B). Calculating P(A) and P(B) counts outcomes that are in both A and B twice, so they're overcounted. The probability of the intersection of the two events, denoted as A&B, must be subtracted from the sum.

P(even number OR number greater than 8) = P(even number) + P(greater than 8) - P(even AND greater than 8) or (18+10-4)/36

The probability of rolling a number greater than 10 is (2+1)/36: The only outcomes greater than 10 are 11 and 12 which are mutually exclusive. The first, 11, can occur in two ways, and the second, 12, can occur only with a roll of double 6's.


### Probability mass functions - discrete variables 

A **random variable** is a numerical outcome of an expiriment. 

Can be discrete (categorical) or continuous (ranges)

A Probability Mass Function (PMF) evaluated at a value corresponds to the probability that a random variable takes that value. To be valid, the function must:

* Be larger than or equal to 0
* The sum of the possible values that the rand variable can take has to add up to 1

Example, the Bernoulli distribution (result of a coin flip):

X = 0 represents tails and X = 1 represents heads

p(x) = (1/2)^x^ (1/2)^1-x^ for x = 0,1

If you plug in 0 and 1 for x you get 1/2. This means that the probability of heads is 1/2 and the probability of tails is 1/2 for a fair coin. 

Suppose we have a coin which may or may not be fair. Let x=0 represent a 'heads' outcome and x=1 represent a 'tails' outcome of a coin toss. If p is the probability of 'heads' which of the following represents thePMF of the coin toss?

(p^(1-x))*(1-p)^x



### Probability denisity functions - continuous variables 

A Probability Denisity Function (PDF) is a function associated with a continuous random variable. 

To be valid, the function must:

* Be larger than or equal to 0 everywhere

* The total area under it must be 1

Areas under PDFs correspond to probabilities for that random variable. 

Example, a right triangle density curve of answered support phone calls:

```{r}
x <- c(-0.5, 0, 1, 1, 1.5)
y <- c(0, 0, 2, 0, 0)
plot(x, y, lwd = 3, frame = FALSE, type = "l")

# note the area under the curve is 1
```

What's the probability that 75% or fewer calls get addressed in a randomly sampled day from this population?

That would be the area under the curve between 0 - 0.75. The height of y when x = 0.75 is 1.5. Since this is a right triangle we can just use the formula to calculate the area of a triangle:

```{r}
1.5 * 0.75/2
```

So the probability is 0.56 or 56%. 

This distribution is also an example of a beta distribution so we can use the pbeta function to predict the probability. Note that p in front of a function asks for the probability. 


```{r}
pbeta(0.75, 2, 1)
```

### Cumulative Distribution and Survival Functions

The cumulative distribution function (CDF) of a random variable, X, returns the probability that the random variable is less than or equal to the value x - whether x is discrete or continuous. 

`pbeta` is the CDF in R?

F(x) = P(X<=x)

When the random variable is continuous,the PDF is the derivative of the CDF. So integrating the PDF (the line represented by the diagonal) yields the CDF. When you evaluate the CDF at the limits of integration the 
result is an area. 


The survival function of a random variable, X, is defined as the probability that the random variable is greater than the value x. 

S(x) = P(X>x)1

Notice that S(x) = 1 - F(x)

Example, what are the survival function and CDF from the density considered before? Whats the probability that 40%, 50%, and 60% or fewer get ansered on a random day? 

```{r}
pbeta(c(0.4, 0.5, 0.6), 2, 1)
```

The probability that 40% or fewer is 16%, 50% or fewer is 25%, and 60% or fewer is 36%. 

### Quantiles

An example of a sample quantile is scoring in the nth percentage on an exam. You know that you scored better than n% and (100-n)% scored better than you. 

The $\alpha$^th^ quantile of a distribution with distribution function F is the point 
$x_{\alpha}$ so that F($x_{\alpha}$) = $\alpha$

A percentile is simply a quantile with $\alpha$ expressed as a percent

The median is the 50^th^ percentile 

For example, in a density curve, if x = 0.95, or is in the 95th percentile, about 95% of the observations that we draw from the population will be less than x and about 5% will be larger. 

The sample is the estimator, the population (i.e. what we're infering about) is the estimand. 

## Comparisons: PMF, PDF, CDF

From: https://www.youtube.com/watch?v=1xQ4r2gcW3c

Distributions characterize random variables. Random variables are either discrete (PMF) or continuous (PDF). About these distributions, we can ask either an "equal to" (PDF/PMF) question or a "less than" question (CDF). But all distributions have the same job: characterize the random variable.

            Density     Cumulative

Discrete:    PMF         CDF

Continuous:  PDF         CDF


Types of Distributions:

* Uniform: flat, each outcome is equally likely (EX. rolling a single die)
* Normal: curved (EX. rolling many dice and adding up)
* Poisson: (EX. process breakdown)

PDF requires an interval: P(x1 < X < x2)

PMF is exact: P(X = x)

CDF vs. PDF: CDF is less than or equal to while PDF is approximiately equal

CDF: total area under the curve to the left as a percentage 

## Conditional Probability









